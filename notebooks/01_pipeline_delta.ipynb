{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01a2f5ee-e77f-44f7-9648-2ba95b302470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "%pip install pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08895a8f-0d36-429d-b67d-8965be73d8e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "import re\n",
    "import pypdf\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    text: str\n",
    "    page_index: int  # 0-based\n",
    "    page_number: int # 1-based\n",
    "\n",
    "\n",
    "def load_pdf_and_chunk(pdf_path: str, max_chars: int = 1200) -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    PDF 1ファイルを複数チャンク（Chunk オブジェクトのリスト）にして返す簡易版。\n",
    "    - pypdf でページごとにテキスト抽出\n",
    "    - 空行で段落に分割\n",
    "    - max_chars 文字まで段落を詰め込んで 1 チャンク\n",
    "    \"\"\"\n",
    "    reader = pypdf.PdfReader(pdf_path)\n",
    "    chunks: List[Chunk] = []\n",
    "\n",
    "    for page_idx, page in enumerate(reader.pages):\n",
    "        raw_text = page.extract_text() or \"\"\n",
    "        # 連続改行を 2 つまでに圧縮\n",
    "        normalized = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", raw_text.strip())\n",
    "\n",
    "        # 空行で段落分割\n",
    "        paragraphs = [p.strip() for p in normalized.split(\"\\n\\n\") if p.strip()]\n",
    "        if not paragraphs:\n",
    "            continue\n",
    "\n",
    "        buf = \"\"\n",
    "        for para in paragraphs:\n",
    "            # 段落を足しても max_chars 以内ならバッファに詰める\n",
    "            if len(buf) + len(para) + 2 <= max_chars:\n",
    "                buf = (buf + \"\\n\\n\" + para) if buf else para\n",
    "            else:\n",
    "                # いったんチャンクとして確定\n",
    "                chunks.append(\n",
    "                    Chunk(\n",
    "                        text=buf,\n",
    "                        page_index=page_idx,\n",
    "                        page_number=page_idx + 1,\n",
    "                    )\n",
    "                )\n",
    "                buf = para  # 次のチャンクを開始\n",
    "\n",
    "        # 残りがあればチャンクとして追加\n",
    "        if buf:\n",
    "            chunks.append(\n",
    "                Chunk(\n",
    "                    text=buf,\n",
    "                    page_index=page_idx,\n",
    "                    page_number=page_idx + 1,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6613cb6-ba10-4921-aec3-d485a0302e5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from pathlib import Path\n",
    "from pyspark.sql import Row\n",
    "\n",
    "PDF_DIR = \"/dbfs/bronze/jsai2025/pdfs\"\n",
    "pdf_paths = [str(p) for p in Path(PDF_DIR).glob(\"*.pdf\")]\n",
    "\n",
    "# Delta の出力先（好きな場所に変えてOK）\n",
    "CHUNK_TEXT_DIR = \"dbfs:/silver/jsai2025/chunks_text\"\n",
    "\n",
    "\n",
    "def split_pdf_to_chunks(pdf_path: str):\n",
    "    # ★ さっき定義した load_pdf_and_chunk をそのまま呼ぶ\n",
    "    chunks = load_pdf_and_chunk(pdf_path)\n",
    "\n",
    "    rows = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        rows.append(\n",
    "            Row(\n",
    "                pdf_path=pdf_path,\n",
    "                chunk_id=i,                    # PDF 内での通し番号\n",
    "                page_index=chunk.page_index,   # 0 始まり\n",
    "                page_number=chunk.page_number, # 1 始まり\n",
    "                text=chunk.text,\n",
    "            )\n",
    "        )\n",
    "    return rows\n",
    "\n",
    "\n",
    "# PDF パスを RDD にばらまき、flatMap で 1 PDF → 複数チャンクに展開\n",
    "pdf_rdd = spark.sparkContext.parallelize(pdf_paths, numSlices=len(pdf_paths))\n",
    "\n",
    "chunk_df = pdf_rdd.flatMap(split_pdf_to_chunks).toDF()\n",
    "\n",
    "display(chunk_df)\n",
    "\n",
    "# ★ Delta / Parquet として保存（ここが「新しいコンテナに書き込む」部分）\n",
    "(\n",
    "    chunk_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"delta\")       # parquet でも OK\n",
    "    .save(CHUNK_TEXT_DIR)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_pipeline_delta",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
