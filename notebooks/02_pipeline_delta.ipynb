{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a892f43-9e26-483e-80ee-d16c58b9098f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6d37c29-206f-46d4-97df-9a3ee63da35a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"AOAI_ENDPOINT\"] = \"AOAI_Endpoint\"\n",
    "os.environ[\"AOAI_API_KEY\"] = \"AOAI_API_KEY\"\n",
    "os.environ[\"AOAI_EMBED_DEPLOYMENT\"] = \"text-embedding-3-small\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61321361-4972-4b66-8667-ea6274fbc665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "AOAI_ENDPOINT         = os.environ[\"AOAI_ENDPOINT\"]\n",
    "AOAI_API_KEY          = os.environ[\"AOAI_API_KEY\"]\n",
    "AOAI_EMBED_DEPLOYMENT = os.environ[\"AOAI_EMBED_DEPLOYMENT\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eba9dc6d-102b-40ce-8453-9a0cad705963",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType\n",
    "\n",
    "CHUNK_TEXT_DIR  = \"dbfs:/silver/jsai2025/chunks_text\"\n",
    "CHUNK_EMBED_DIR = \"dbfs:/silver/jsai2025/chunks_embed\"\n",
    "\n",
    "# ステージ1（チャンク）の出力を読込\n",
    "chunk_df = (\n",
    "    spark.read\n",
    "         .format(\"delta\")\n",
    "         .load(CHUNK_TEXT_DIR)\n",
    ")\n",
    "\n",
    "# 出力スキーマ\n",
    "embed_schema = StructType([\n",
    "    StructField(\"pdf_path\",  StringType(), False),\n",
    "    StructField(\"chunk_id\",  IntegerType(), False),\n",
    "    StructField(\"text\",      StringType(), False),\n",
    "    StructField(\"embedding\", ArrayType(FloatType()), False),\n",
    "])\n",
    "\n",
    "def embed_partition(rows_iter):\n",
    "    \"\"\"\n",
    "    1パーティション単位で Azure OpenAI embedding を呼ぶ。\n",
    "    長すぎるテキストはログを出してスキップする簡易版。\n",
    "    \"\"\"\n",
    "    from openai import AzureOpenAI\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "        api_key=AOAI_API_KEY,\n",
    "        azure_endpoint=AOAI_ENDPOINT,\n",
    "        api_version=\"2024-02-15-preview\",\n",
    "    )\n",
    "\n",
    "    MAX_CHARS = 8000  # ざっくりの上限（必要に応じて調整）\n",
    "\n",
    "    for row in rows_iter:\n",
    "        # まず text を正規化\n",
    "        text = row.text or \"\"\n",
    "        text = text.strip()\n",
    "\n",
    "        # ★ ここで長すぎるチャンクをスキップ\n",
    "        if len(text) > MAX_CHARS:\n",
    "            print(\n",
    "                f\"[WARN] skip long text: \"\n",
    "                f\"pdf_path={row.pdf_path}, chunk_id={row.chunk_id}, len={len(text)}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # ここから先は「許容サイズのテキスト」だけが来る\n",
    "        resp = client.embeddings.create(\n",
    "            model=AOAI_EMBED_DEPLOYMENT,  # text-embedding-3-small のデプロイ名\n",
    "            input=text,\n",
    "        )\n",
    "        emb = resp.data[0].embedding  # list[float]\n",
    "\n",
    "        yield Row(\n",
    "            pdf_path=row.pdf_path,\n",
    "            chunk_id=int(row.chunk_id),\n",
    "            text=row.text,\n",
    "            embedding=emb,\n",
    "        )\n",
    "\n",
    "chunk_df_repart = chunk_df.repartition(32, \"pdf_path\")\n",
    "\n",
    "embedded_rdd = chunk_df_repart.rdd.mapPartitions(embed_partition)\n",
    "embedded_df  = spark.createDataFrame(embedded_rdd, schema=embed_schema)\n",
    "\n",
    "display(embedded_df.limit(5))\n",
    "\n",
    "(\n",
    "    embedded_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"delta\")\n",
    "    .save(CHUNK_EMBED_DIR)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_pipeline_delta",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
