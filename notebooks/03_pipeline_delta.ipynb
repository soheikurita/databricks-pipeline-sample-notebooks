{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1b20128-5655-497b-88c6-861c0d2618a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"aoai.endpoint\", \"AOAI_Endpoint\")\n",
    "spark.conf.set(\"aoai.api_key\",  \"AOAI_API_KEY\")\n",
    "spark.conf.set(\"aoai.gpt_deployment\", \"gpt-5-mini\")\n",
    "spark.conf.set(\"aoai.embed_deployment\", \"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58c65bff-3f4e-4fa5-9405-7150f2055ca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "AOAI_ENDPOINT       = spark.conf.get(\"aoai.endpoint\")\n",
    "AOAI_API_KEY        = spark.conf.get(\"aoai.api_key\")\n",
    "AOAI_GPT_DEPLOYMENT = spark.conf.get(\"aoai.gpt_deployment\")\n",
    "\n",
    "print(AOAI_ENDPOINT, AOAI_GPT_DEPLOYMENT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a9827bb-7ecd-49d4-a54e-8ec72feb0013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "CHUNK_TEXT_DIR    = \"dbfs:/silver/jsai2025/chunks_text\"\n",
    "CHUNK_SUMMARY_DIR = \"dbfs:/silver/jsai2025/chunks_summary\"\n",
    "\n",
    "# 入力\n",
    "chunk_df = (\n",
    "    spark.read\n",
    "         .format(\"delta\")\n",
    "         .load(CHUNK_TEXT_DIR)\n",
    ")\n",
    "\n",
    "# ★ driver で conf を読む（ここでは SparkContext OK）\n",
    "AOAI_ENDPOINT      = spark.conf.get(\"aoai.endpoint\")\n",
    "AOAI_API_KEY       = spark.conf.get(\"aoai.api_key\")\n",
    "AOAI_GPT_DEPLOYMENT = spark.conf.get(\"aoai.gpt_deployment\")\n",
    "\n",
    "# 出力スキーマ\n",
    "summary_schema = StructType([\n",
    "    StructField(\"pdf_path\", StringType(), False),\n",
    "    StructField(\"chunk_id\", IntegerType(), False),\n",
    "    StructField(\"text\",     StringType(), False),\n",
    "    StructField(\"summary\",  StringType(), False),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfe8b0e6-8a5a-4e53-a8a6-3c651a760c17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "def gpt_partition(\n",
    "    rows_iter,\n",
    "    endpoint=AOAI_ENDPOINT,\n",
    "    api_key=AOAI_API_KEY,\n",
    "    deployment=AOAI_GPT_DEPLOYMENT,\n",
    "):\n",
    "    \"\"\"\n",
    "    各パーティション内で GPT-5-mini を 1 クライアントだけ作り、\n",
    "    row.text を要約して Row(...) を返す。\n",
    "    \"\"\"\n",
    "    from openai import AzureOpenAI\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "        api_key=api_key,\n",
    "        azure_endpoint=endpoint,\n",
    "        api_version=\"2024-02-15-preview\",  # 必要に応じて合わせる\n",
    "    )\n",
    "\n",
    "    MAX_CHARS = 6000  # 念のため context 長超え対策\n",
    "\n",
    "    for row in rows_iter:\n",
    "        text = row.text or \"\"\n",
    "        if len(text) > MAX_CHARS:\n",
    "            print(\n",
    "                f\"[WARN] skip long text: pdf_path={row.pdf_path}, \"\n",
    "                f\"chunk_id={row.chunk_id}, len={len(text)}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # プロンプトはお好みで\n",
    "        prompt = (\n",
    "            \"以下は学会論文の一部です。日本語で3行以内に要約してください。\\n\\n\"\n",
    "            f\"{text}\"\n",
    "        )\n",
    "\n",
    "        # ★ Azure OpenAI Responses API を使用（GPT-5-mini 向け）\n",
    "        resp = client.responses.create(\n",
    "            model=deployment,\n",
    "            input=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "\n",
    "        summary_text = resp.output[0].content[0].text  # SDK バージョンにより多少違うかも\n",
    "\n",
    "        yield Row(\n",
    "            pdf_path=row.pdf_path,\n",
    "            chunk_id=int(row.chunk_id),\n",
    "            text=text,\n",
    "            summary=summary_text,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "632e21da-6288-4af8-baa8-fbfb15804624",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openai==1.55.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b67621e0-58f2-4e9a-adc4-1fb5658e3387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62d12914-32be-416a-9576-95ac20daaa21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PDF 単位でパーティション分け & 並列度 32 で GPT 呼び出し\n",
    "summary_rdd = (\n",
    "    chunk_df\n",
    "    .repartition(32, \"pdf_path\")\n",
    "    .rdd\n",
    "    .mapPartitions(gpt_partition)\n",
    ")\n",
    "\n",
    "summary_df = spark.createDataFrame(summary_rdd, schema=summary_schema)\n",
    "\n",
    "display(summary_df.limit(5))\n",
    "\n",
    "(\n",
    "    summary_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"delta\")\n",
    "    .save(CHUNK_SUMMARY_DIR)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_pipeline_delta",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
